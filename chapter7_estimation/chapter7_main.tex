\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}    
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\ttfamily\scriptsize, % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{Blue},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it is 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{backcolour},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=false,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  keywordstyle=\color{RoyalBlue},      % keyword style
  commentstyle=\color{YellowGreen},   % comment style
  stringstyle=\color{ForestGreen}      % string literal style
} 


%-------------------------------------------------------------

\title{Chapter 7 Estimation}
\author{Yurun (Ellen) Ying}
\date{Sep 27, 2022}
\begin{document}
\maketitle


%-------------------------------------------------------------
\section*{7.2 Prior and Posterior Distributions}

\subsection*{Problem 1}
Consider again the situation described in Example 7.2.8. This time, suppose that the experimenter believes that the prior distribution of $\theta$ is the gamma distribution with parameters 1 and 5000. What would this experimenter compute as the value of $\Pr(X_6 > 3000 \mid \boldsymbol{x})$?

\

\textbf{Answer:} Based on the given conditions, the prior p.d.f. for $\theta > 0$ is given by
\begin{equation*}
\xi (\theta) = 5000 \cdot e^{-5000 \theta}
\end{equation*}
Also, $\xi (\theta) = 0$ for $\theta \le 0$. The p.d.f. for each $X_i$ conditioned on $\theta$ is given by 
\begin{equation*}
f(x \mid \theta) = \theta e^{-\theta x}
\end{equation*}
for $x > 0$. Therefore, the conditional joint p.d.f. for the first five florescent lights is given by
\begin{align*}
f_n(\boldsymbol{x} \mid \theta) &= \prod_i^n f(x_i \mid \theta) \\
&= \theta^n e^{-\theta y}
\end{align*}
where $y = \sum_i^n x_i$.\\
From $\xi(\theta \mid \boldsymbol{x}) \propto f(\boldsymbol{x} \mid \theta) \xi(\theta)$, we have
\begin{equation*}
\xi(\theta \mid \boldsymbol{x}) \propto \theta^n e^{-\theta(5000+y)}
\end{equation*}
We can now compute $g_n(\boldsymbol{x})$, which is given by
\begin{equation*}
g_n(\boldsymbol{x}) = \int_{0}^{\infty} \theta^n e^{-\theta(5000+y)} d \theta = \frac{\Gamma(n + 1)}{(y + 5000)^{n+1}}
\end{equation*}
From the previous two equations, we have
\begin{equation*}
\xi(\theta \mid \boldsymbol{x}) = \frac{(y + 5000)^{n+1}}{\Gamma(n + 1)} \theta^n e^{-\theta(5000+y)}
\end{equation*}
When $n = 5$ and $y = 16,178$, this equation becomes
\begin{equation*}
\xi(\theta \mid x_1,\dots,x_5) = \frac{(21,178)^{6}}{\Gamma(6)} \theta^5 e^{-21,178\theta}
\end{equation*}\\
To compute $\Pr(X_6 > 3000 \mid \boldsymbol{x})$, we first need the p.d.f. of $X_6$ conditioned on the $X_1, \dots, X_5$,
\begin{align*}
f(x_6 \mid x_1,\dots,x_5) &= \int_0^{\infty} f(x_6 \mid \theta) \xi(\theta \mid x_1,\dots,x_5)\\
&= \frac{(21,178)^{6}}{\Gamma(6)} \int_0^{\infty} \theta^{6} e^{-(x_6 + 21,178) \theta} d\theta \\
&= \frac{6 (21,178)^{6}}{(x_6 + 21,178)^{7}}.
\end{align*}
Therefore, we have 
\begin{equation*}
\Pr(X_6 > 3000 \mid \boldsymbol{x}) = 6 (21,178)^{6} \int_{3000}^{\infty} \frac{1}{(x_6 + 21,178)^{7}} dx_6 \approx 0.4516.
\end{equation*} 

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 3}
Suppose that the number of defects on a roll of magnetic recording tape has a Poisson distribution for which the mean $\lambda$ is either 1.0 or 1.5, and the prior p.f. of $\lambda$ is as follows:
\begin{equation*}
\xi(1.0) = 0.4 \; \text{and} \; \xi(1.5) = 0.6.
\end{equation*}
If a roll of tape selected at random is found to have three defects, what is the posterior p.f. of $\lambda$?

\

\textbf{Answer:} Based on the information given, the conditional p.d.f. of the $X$, the number of defects, can be written as
\begin{equation*}
p(x \mid \lambda) = e^{-\lambda}\frac{\lambda^x}{x!},
\end{equation*}
for $x \le 0$ and where $\lambda > 0$.
To compute the posterior p.f. of $\lambda$, we first need the marginal distribution of $X$,
\begin{equation*}
g(x) = p(x \mid 1.0) \, \xi(1.0) + p(x \mid 1.5) \, \xi(1.5) = \frac{0.4e^{-1} + 0.5 \cdot 1.5^x e^{-1.5}}{x!}.
\end{equation*}
Therefore, the posterior distribution of $X$ is given by
\begin{equation*}
\xi(1.0 \mid X = 3) = \frac{p(X = 3 \mid 1.0) \, \xi(1.0)}{g(3)} \approx 0.2457
\end{equation*}
and
\begin{equation*}
\xi(1.5 \mid X = 3) = \frac{p(X = 3 \mid 1.5) \, \xi(1.5)}{g(3)} \approx 0.7543
\end{equation*}

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 5}
Suppose that the prior distribution of some parameter $\theta$ is a beta distribution for which the mean is 1/3 and the variance is 1/45. Determine the prior p.d.f. of $\theta$.

\

\textbf{Answer:} Based on the information given, we have
\begin{equation*}
	\begin{cases}
		\frac{\alpha}{\alpha + \beta} = \frac{1}{3}\\
		\frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} = \frac{1}{45}
	\end{cases},
\end{equation*}
which gives $\alpha = 3$ and $\beta = 6$. Therefore, the prior distribution of $\theta$ is
\begin{equation*}
\xi(\theta) = \frac{\theta^3 \, (1 - \theta)^6}{\text{B}(3,6)},
\end{equation*}
where 
\begin{equation*}
\text{B}(3,6) = \frac{\Gamma(3) \, \Gamma(6)}{\Gamma(9)},
\end{equation*}
which is the p.d.f. of a beta distribution with parameters 3 and 6.


%-------------------------------------------------------------
\bigskip

\subsection*{Problem 7}
Suppose that the proportion $\theta$ of defective items in a large manufactured lot is unknown, and the prior distribution of $\theta$ is given by
\begin{equation*}
	\xi(\theta) = \begin{cases}
		2\,(1 - \theta) \;\; \text{for} \: 0 < \theta < 1 \\
		0 \;\; \text{otherwise}
	\end{cases}.
\end{equation*}
When eight items are selected at random from the lot, it is found that exactly three of them are defective. Determine the posterior distribution of $\theta$. 

\

\textbf{Answer:} To determine the posterior distribution of $\theta$, we first need to compute the conditional joint distribution of the result of first $n$ draws of items, with each denoted by $X_i$. Note that each $X_i$ has a bernoulli distribution with a conditional p.f. $p(x_i \mid \theta) = \theta^{x_i} \, (1 - \theta)^{1-x_i}$. Since each draw is independent (assuming the total number of items is very large), we can write the conditional joint distribution as 
\begin{equation*}
p_n(\boldsymbol{x} \mid \theta) = \sum_i^n p(x_i \mid \theta) = \theta^{d} \, \theta^{1-d},
\end{equation*}
where $d = \sum_i^n x_i$, the number of defect items in the first $n$ draws.\\
We now have 
\begin{align*}
\xi(\theta \mid x_1,\dots,x_8) &\propto p_8(\boldsymbol{x} \mid \theta) \, \xi(\theta) \\
& \propto \theta^3 \, (1 - \theta)^6.
\end{align*}
The proportionality constant is given by
\begin{equation*}
\int_0^1 \theta^3 \, (1 - \theta)^6 d \theta = \text{B}(4,\,7).
\end{equation*}
Therefore,
\begin{equation*}
\xi(\theta \mid x_1,\dots,x_8) = \frac{1}{\text{B}(4,\,7)} \, \theta^3 \, (1 - \theta)^6,
\end{equation*}
which is the p.d.f. of a beta distribution with parameters 4 and 7.

%-------------------------------------------------------------
\newpage
\section*{7.3 Conjugate Prior Distribution}

\subsection*{Problem 1}
Consider again the situation described in Example 7.3.10. Once again, suppose that the prior distribution of $\theta$ is a normal distribution with mean 0, but this time let the prior variance be $v^2 > 0$. If the posterior mean of $\theta$ is 0.12, what value of $v^2$ was used?

\

\textbf{Answer:} From the information given, we have the following equation:
\begin{equation*}
\frac{\sigma^2 \mu_0 + n v^2 \overline{x}_n}{\sigma^2 + n v^2} = 0.12,
\end{equation*}
where $\sigma^2 = 100$, $\mu_0 = 0$, $\overline{x}_n = 0.125$, and $n = 20$. Solving for $v^2$ yields $v^2 = 120$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 3}
Suppose that the proportion $\theta$ of defective items in a large shipment is unknown and that the prior distribution of $\theta$ is the beta distribution with parameters 2 and 200. If 100 items are selected at random from the shipment and if three of these items are found to be defective, what is the posterior distribution of $\theta$?

\

\textbf{Answer:} The posterior distribution of $\theta$ is a beta distribution with parameter $\alpha = 5$ and $\beta = 297$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 5}
Suppose that the number of defects in a 1200-foot roll of magnetic recording tape has a Poisson distribution for which the value of the mean $\theta$ is unknown and that the prior distribution of $\theta$ is the gamma distribution with parameters $\alpha = 3$ and $\beta = 1$. When five rolls of this tape are selected at random and inspected, the numbers of defects found on the rolls are 2, 2, 6, 0, and 3. Determine the posterior distribution of $\theta$.

\

\textbf{Answer:} The posterior distribution is a gamma distribution with parameter $\alpha = 16$ and $\beta = 6$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 7}
Suppose that the heights of the individuals in a certain population have a normal distribution for which the value of the mean $\theta$ is unknown and the standard deviation is 2 inches. Suppose also that the prior distribution of $\theta$ is a normal distribution for which the mean is 68 inches and the standard deviation is 1 inch. If 10 people are selected at random from the population, and their average height is found to be 69.5 inches, what is the posterior distribution of $\theta$?

\

\textbf{Answer:} From the information given, we have 
\begin{equation*}
\mu_1 = \frac{4 \times 68 + 10 \times 1 \times 69.5}{4 + 10 \times 1} = \frac{967}{14} \; \text{and} \;
v_1^2 = \frac{4 \times 1}{4 + 10 \times 1} = \frac{2}{7}.
\end{equation*}
Therefore, the posterior distribution of $\theta$ is a normal distribution with $\mu_1 = \frac{967}{14}$ and $v_1^2 = \frac{2}{7}$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 13}
Suppose that the time in minutes required to serve a customer at a certain facility has an exponential distribution for which the value of the parameter $\theta$ is unknown.\\
For a distribution with mean $\mu \ne 0$ and standard deviation $\sigma > 0$, the \textit{coefficient of variation} of the distribution is defined as $\sigma/|\mu|$. Suppose that the coefficient of variation of the prior gamma distribution of $\theta$ is 2. What is the smallest number of customers that must be observed in order to reduce the coefficient of variation of the posterior distribution to 0.1?

\textbf{Answer:} The coefficient of variation of the prior gamma distribution is
\begin{equation*}
\frac{v_0}{\mu_0} = \frac{\sqrt{\alpha_0}/\beta_0}{\alpha_0/\beta_0} = \frac{1}{\sqrt{\alpha_0}} = 2,
\end{equation*}
which gives $\alpha_0 = \frac{1}{4}$. By the same token, the coefficient of variation of the posterior distribution is 
\begin{equation*}
\frac{1}{\sqrt{\alpha_1}} = \frac{1}{\sqrt{\alpha_0 + n}}.
\end{equation*}
To reduce this value to 0.1, the value of $\alpha_0 + n$ should at least be 100. This means that $n$ at least needs to be 100.

%-------------------------------------------------------------
\newpage

\section*{7.4 Bayes Estimator}
\subsection*{Problem 1}
In a clinical trial, let the probability of successful outcome $\theta$ have a prior distribution that is the uniform distribution on the interval [0, 1], which is also the beta distribution with parameters 1 and 1. Suppose that the first patient has a successful outcome. Find the Bayes estimates of $\theta$ that would be obtained for both the squared error and absolute error loss functions.

\

\textbf{Answer:} When the squared error loss function is used, the Bayes estimator is the mean of the posterior distribution of $\theta$, which is a beta distribution with parameter $\alpha_1 = 2$ and $\beta_1 = 1$. We have the Bayes estimator as follows,
\begin{equation*}
\delta^*(\boldsymbol{X}) = \mathbb{E}[\theta \mid \boldsymbol{x}] = \frac{\alpha_0 + \sum_i^n X_i}{\alpha_0 + \beta_0 + n}.
\end{equation*}
Based on the information given, $\sum X_i = 1$ and $n = 1$. Therefore, $\delta^*(\boldsymbol{x}) = \frac{2}{3}$.\\
When the absolute error loss function is used, the Bayes estimator is the median of the posterior distribution of $\theta$. Note that the p.d.f. of the posterior distribution of $\theta$ is
\begin{equation*}
\xi(\theta \mid \boldsymbol{x}) = \frac{\theta}{\text{B}(2,1)},
\end{equation*}
and there for the cumulative distribution function is
\begin{equation*}
F_{\theta \mid \boldsymbol{x}}(x) = \frac{1}{\text{B}(2,1)} \int_0^x \theta d\theta.
\end{equation*}
Set this function equal to 0.5 and solve the equation, which gives $x = \frac{\sqrt{2}}{2}$. This means that the Bayes estimate of $\theta$ when absolute error loss function is used is $2^{-1/2}$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 3}
Suppose that the proportion $\theta$ of defective items in a large shipment is unknown, and the prior distribution of $\theta$ is the beta distribution for which the parameters are $\alpha = 5$ and $\beta = 10$. Suppose also that 20 items are selected at random from the shipment.\\
a. For what number of defective items in the sample will the mean squared error of the Bayes estimate be a maximum?\\
b. For what number will the mean squared error of the Bayes estimate be a minimum?

\

\textbf{Answer:} From the information given, the posterior distribution of $\theta$ will be a beta distribution with parameters $\alpha_1 = 5 + y$ and $\beta_1 = 30 - y$, where $y$ is the number of defective items among the sample of 20 items and $0 \le y \le 20$. We have the mean squared error being
\begin{equation*}
\text{Var}(\theta \mid \boldsymbol{x}) = \frac{\alpha_1 \beta_1}{(\alpha_1 + \beta_1)^2 (\alpha_1 + \beta_1 + 1)} = \frac{(5 + y)(30-y)}{35^2 \times 36} = \frac{- (y - 25/2)^2 + c}{35^2 \times 36}.
\end{equation*}
The estimator is maximized when $y = 12 \; \text{or} \; 13$ and minimized when $y = 0$.

%-------------------------------------------------------------
\bigskip

\subsection*{Problem 5}
Suppose that the number of defects in a 1200-foot roll of magnetic recording tape has a Poisson distribution for which the value of the mean $\theta$ is unknown, and the prior distribution of $\theta$ is the gamma distribution with parameters $\alpha = 3$ and $\beta = 1$. When five rolls of this tape are selected at random and inspected, the numbers of defects found on the rolls are 2, 2, 6, 0, and 3. If the squared error loss function is used, what is the Bayes estimate of $\theta$?

\

\textbf{Answer:} Based on the information given, the posterior distribution of $\theta$ is a gamma distribution with parameters $\alpha_1 = 16$ and $\beta_1 = 6$. Therefore, we have the Bayes estimator being
\begin{equation*}
\delta^*{X} = \frac{\alpha_1}{\beta_1} = \frac{8}{3}.
\end{equation*}


\subsection*{Problem 9}
Suppose that a random sample is to be taken from a normal distribution for which the value of the mean $\theta$ is unknown and the standard deviation is 2, the prior distribution of $\theta$ is a normal distribution for which the standard deviation is 1, and the value of $\theta$ must be estimated by using the squared error loss function. What is the smallest random sample that must be taken in order for the mean squared error of the Bayes estimator of $\theta$ to be 0.01 or less?

\

\textbf{Answer:} The mean squared error of the Bayes estimator of $\theta$ is given by
\begin{align*}
\mathbb{E}[L(\theta, \delta^*(\boldsymbol{x})) \mid \boldsymbol{x}] &= \mathbb{E}[(\theta - \delta^*(\boldsymbol{x}))^2 \mid \boldsymbol{x}] \\
&= \mathbb{E}[\theta^2 \mid \boldsymbol{x}] - 2 \delta^*(\boldsymbol{x}) \mathbb{E}[\theta \mid \boldsymbol{x}] + (\delta^*(\boldsymbol{x}))^2 \\
&= \mathbb{E}[\theta^2 \mid \boldsymbol{x}] - (\mathbb{E}[\theta \mid \boldsymbol{x}])^2 \\
&= \text{Var}[\theta \mid \boldsymbol{x}] \\
&= \frac{\sigma^2 v_0^2}{\sigma^2 + n v_0^2} = \frac{4}{4 + n}.
\end{align*}
This value being less than or equal to 0.01 gives the minimum value of $n = 396$.

%-------------------------------------------------------------
\newpage


\end{document}